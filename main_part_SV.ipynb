{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import lightning\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC\n",
    "\n",
    "from source.preprocessing import hdf5_to_seq\n",
    "from source.part import ParticleTransformer\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "rnd_seed = 42\n",
    "lightning.seed_everything(rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWoLaDataset(Dataset):\n",
    "    def __init__(self, signal: torch.Tensor, background: torch.Tensor, CWoLa_ratio: float=0):\n",
    "        self.x = torch.cat([signal, background], dim=0)\n",
    "\n",
    "        # Randomly assign labels to a fraction of the data\n",
    "        num_sig = len(signal)\n",
    "        num_bkg = len(background)\n",
    "        sig_y = torch.zeros(num_sig)\n",
    "        bkg_y = torch.ones(num_bkg)\n",
    "        sig_y[torch.randperm(num_sig)[:int(CWoLa_ratio * num_sig)]] = 1\n",
    "        bkg_y[torch.randperm(num_bkg)[:int(CWoLa_ratio * num_bkg)]] = 0\n",
    "        self.y = torch.cat([sig_y, bkg_y], dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class LitDataModule(lightning.LightningDataModule):\n",
    "    def __init__(self, batch_size=64, val_split=0.1, CWoLa_ratio: float=0):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        \n",
    "        GGF = hdf5_to_seq('GGF.h5')\n",
    "        VBF = hdf5_to_seq('VBF.h5')\n",
    "\n",
    "        # Generate indices\n",
    "        num_ggf = len(GGF)\n",
    "        num_vbf = len(VBF)\n",
    "\n",
    "        ggf_train_len = int((1 - self.val_split) * num_ggf)\n",
    "        vbf_train_len = int((1 - self.val_split) * num_vbf)\n",
    "\n",
    "        ggf_indices = torch.randperm(num_ggf)\n",
    "        vbf_indices = torch.randperm(num_vbf)\n",
    "\n",
    "        GGF_train = GGF[ggf_indices[:ggf_train_len]]\n",
    "        GGF_valid = GGF[ggf_indices[ggf_train_len:]]\n",
    "\n",
    "        VBF_train = VBF[vbf_indices[:vbf_train_len]]\n",
    "        VBF_valid = VBF[vbf_indices[vbf_train_len:]]\n",
    "\n",
    "        # class_counts: 0 = negative class (GGF), 1 = positive class (VBF)\n",
    "        self.pos_weight = torch.tensor([ggf_train_len / vbf_train_len], dtype=torch.float)\n",
    "\n",
    "        # Create datasets\n",
    "        self.train_dataset = CWoLaDataset(GGF_train, VBF_train, CWoLa_ratio)\n",
    "        self.valid_dataset = CWoLaDataset(GGF_valid, VBF_valid)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLitModel(lightning.LightningModule):\n",
    "    def __init__(self, model: nn.Module, pos_weight: torch.Tensor):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        self.train_accuracy = BinaryAccuracy()\n",
    "        self.valid_accuracy = BinaryAccuracy()\n",
    "        self.train_auc = BinaryAUROC()\n",
    "        self.valid_auc = BinaryAUROC()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:        \n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RAdam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def _shared_step(self, batch: tuple[torch.Tensor, torch.Tensor], mode: str):\n",
    "        x, y_true = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits.view(-1), y_true.float())\n",
    "        y_pred = torch.sigmoid(logits.view(-1))\n",
    "\n",
    "        # Update metrics\n",
    "        if mode == 'train':\n",
    "            self.train_auc.update(y_pred, y_true)\n",
    "            self.train_accuracy.update(y_pred, y_true)\n",
    "        elif mode == 'valid':\n",
    "            self.valid_auc.update(y_pred, y_true)\n",
    "            self.valid_accuracy.update(y_pred, y_true)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(f\"{mode}_loss\", loss, on_epoch=True, prog_bar=(mode == 'train'))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, mode='valid')\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        auc = self.train_auc.compute()\n",
    "        acc = self.train_accuracy.compute()\n",
    "        self.log('train_auc', auc, prog_bar=True)\n",
    "        self.log('train_accuracy', acc, prog_bar=True)\n",
    "        self.train_auc.reset()\n",
    "        self.train_accuracy.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        auc = self.valid_auc.compute()\n",
    "        acc = self.valid_accuracy.compute()\n",
    "        self.log('valid_auc', auc, prog_bar=True)\n",
    "        self.log('valid_accuracy', acc, prog_bar=True)\n",
    "        self.valid_auc.reset()\n",
    "        self.valid_accuracy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParT_Baseline(ParticleTransformer):\n",
    "    def __init__(self):\n",
    "\n",
    "        hyperparameters = {\n",
    "            \"ParEmbed\": {\n",
    "                \"input_dim\": 3 + 3,  # (pt, eta, phi) + one-hot_encoding\n",
    "                \"embed_dim\": [128, 512, 128]\n",
    "            },\n",
    "            \"ParAtteBlock\": {\n",
    "                \"num_heads\": 8,\n",
    "                \"fc_dim\": 512,\n",
    "                \"dropout\": 0.1\n",
    "            },\n",
    "            \"ClassAtteBlock\": {\n",
    "                \"num_heads\": 8,\n",
    "                \"fc_dim\": 512,\n",
    "                \"dropout\": 0.0\n",
    "            },\n",
    "            \"num_ParAtteBlock\": 8,\n",
    "            \"num_ClassAtteBlock\": 2\n",
    "        }\n",
    "\n",
    "        super().__init__(score_dim=1, parameters=hyperparameters)\n",
    "\n",
    "class ParT_Light(ParticleTransformer):\n",
    "    def __init__(self):\n",
    "\n",
    "        hyperparameters = {\n",
    "            \"ParEmbed\": {\n",
    "                \"input_dim\": 3 + 3,  # (pt, eta, phi) + one-hot_encoding\n",
    "                \"embed_dim\": [64, 64, 64]\n",
    "            },\n",
    "            \"ParAtteBlock\": {\n",
    "                \"num_heads\": 4,\n",
    "                \"fc_dim\": 64,\n",
    "                \"dropout\": 0.1\n",
    "            },\n",
    "            \"ClassAtteBlock\": {\n",
    "                \"num_heads\": 4,\n",
    "                \"fc_dim\": 64,\n",
    "                \"dropout\": 0.0\n",
    "            },\n",
    "            \"num_ParAtteBlock\": 4,\n",
    "            \"num_ClassAtteBlock\": 1\n",
    "        }\n",
    "\n",
    "        super().__init__(score_dim=1, parameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup.\n",
    "CWoLa_ratio = 0.0\n",
    "model = ParT_Light()\n",
    "\n",
    "# Save directory and name.\n",
    "save_dir = os.path.join('training_logs')\n",
    "preprocessing_mode = \"SV\"\n",
    "name = f\"ParT_{preprocessing_mode}_{rnd_seed}\"\n",
    "version = f\"{model.__class__.__name__}_{CWoLa_ratio}\"\n",
    "\n",
    "\"\"\"Training\"\"\"\n",
    "# Lightning DataModule & Model.\n",
    "lit_data_module = LitDataModule(CWoLa_ratio=CWoLa_ratio)\n",
    "lit_model = BinaryLitModel(model=model, pos_weight=lit_data_module.pos_weight)\n",
    "with open(os.path.join(save_dir, name, version, 'num_params.txt'), 'w') as file_num_params:\n",
    "    for depth in range(1, 4):\n",
    "        print(f\"Model Summary (max_depth={depth}):\", file=file_num_params)\n",
    "        print(ModelSummary(lit_model, max_depth=depth), file=file_num_params)\n",
    "        print(f\"\\n{'='*100}\\n\", file=file_num_params)\n",
    "\n",
    "# Lightning Logger & Trainer.\n",
    "logger = CSVLogger(save_dir=save_dir, name=name, version=version)\n",
    "trainer = lightning.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    "    callbacks=[ModelCheckpoint(\n",
    "            monitor='valid_auc',\n",
    "            mode='max',\n",
    "            save_top_k=5,\n",
    "            save_last=True,\n",
    "            filename='{epoch}-{valid_auc:.3f}-{valid_accuracy:.3f}',\n",
    "        )],\n",
    ")\n",
    "\n",
    "# Train and test the model.\n",
    "trainer.fit(lit_model, lit_data_module)\n",
    "trainer.test(lit_model, datamodule=lit_data_module, ckpt_path='best')\n",
    "\n",
    "\"\"\"Plot Metrics\"\"\"\n",
    "metrics_csv = os.path.join(save_dir, name, version, 'metrics.csv')\n",
    "df = pd.read_csv(metrics_csv)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "metrics = ['train_loss_epoch', 'train_accuracy', 'train_auc', 'valid_loss', 'valid_accuracy', 'valid_auc']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    data = df[df[metric].notna()]\n",
    "    plot = sns.lineplot(data=data, x='epoch', y=metric, ax=ax.flat[i])\n",
    "    plot.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, name, version, 'metrics.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
