{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0698be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import h5py\n",
    "import lightning\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    '''Jupyter notebook environment has no __file__ attribute.'''\n",
    "    project_root = Path.cwd().parent\n",
    "sys.path.append(project_root.as_posix())\n",
    "\n",
    "from src.model_cnn import CNN_EventCNN\n",
    "from src.model_part import ParT_Light\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18650227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_pi(phi: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "    \"\"\"Wrap angles to [-pi, pi).\"\"\"\n",
    "    return (phi + np.pi) % (2 * np.pi) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ec348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCSimData:\n",
    "    def __init__(self, path: str | Path, luminosity: float = 1.0):\n",
    "        self.path = str(path)\n",
    "        self.luminosity = luminosity\n",
    "\n",
    "        # -------- Channels --------\n",
    "        self.detector_channels = ['TOWER', 'TRACK']\n",
    "        self.slices = [slice(0, 250), slice(250, 400)]\n",
    "        if 'diphoton' in self.path:\n",
    "            self.decay_channel = 'PHOTON'\n",
    "            self.slices.append(slice(400, 402))\n",
    "        elif 'zz4l' in self.path:\n",
    "            self.decay_channel = 'LEPTON'\n",
    "            self.slices.append(slice(400, 404))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {self.path}. Supported datasets are 'diphoton' and 'zz4l'.\")\n",
    "        self.channels = self.detector_channels + ([self.decay_channel] if self.decay_channel else [])\n",
    "\n",
    "        with h5py.File(str(path), 'r') as hdf5_file:\n",
    "            # -------- Jet flavor --------\n",
    "            self.jet_flavor = self._extract_jet_flavor(hdf5_file)\n",
    "\n",
    "            # -------- Particle flow information (pt, eta, phi) --------\n",
    "            particle_flow = self._extract_particle_flow(hdf5_file, self.channels)\n",
    "\n",
    "            # -------- Preprocessing --------\n",
    "            particle_flow = self._preprocess_phi_transformation(particle_flow)\n",
    "            particle_flow = self._preprocess_center_of_phi(particle_flow)\n",
    "            particle_flow = self._preprocess_flipping(particle_flow)\n",
    "\n",
    "        self.particle_flow = particle_flow\n",
    "\n",
    "    def _extract_jet_flavor(self, hdf5_file: h5py.File) -> Dict[str, np.ndarray | int]:\n",
    "        \"\"\"Build gluon/quark composition masks from J1/J2 flavors.\"\"\"\n",
    "\n",
    "        J1 = np.asarray(hdf5_file[\"J1\"][\"flavor\"][:])\n",
    "        J2 = np.asarray(hdf5_file[\"J2\"][\"flavor\"][:])\n",
    "        g1, g2 = (J1 == 21), (J2 == 21)  # 21 == gluon\n",
    "\n",
    "        mask_2q0g = (~g1) & (~g2)\n",
    "        mask_1q1g = ((~g1) & g2) | (g1 & (~g2))\n",
    "        mask_0q2g = g1 & g2\n",
    "\n",
    "        return {\n",
    "            \"2q0g\": mask_2q0g,\n",
    "            \"1q1g\": mask_1q1g,\n",
    "            \"0q2g\": mask_0q2g,\n",
    "            \"total\": len(J1),\n",
    "        }\n",
    "    \n",
    "    def _extract_particle_flow(self, hdf5_file: h5py.File, channels: List[str]) -> np.ndarray:\n",
    "        \"\"\"Load pt/eta/phi/(mask) for each channel.\"\"\"\n",
    "\n",
    "        # -------- Particle flow array (N, ΣM, 4) --------\n",
    "        pts = np.concatenate([np.asarray(hdf5_file[channel]['pt'][:],  dtype=np.float32) for channel in channels], axis=1)  # (N, ΣM)\n",
    "        etas = np.concatenate([np.asarray(hdf5_file[channel]['eta'][:], dtype=np.float32) for channel in channels], axis=1)  # (N, ΣM)\n",
    "        phis = np.concatenate([np.asarray(hdf5_file[channel]['phi'][:], dtype=np.float32) for channel in channels], axis=1)  # (N, ΣM)\n",
    "        phis = wrap_pi(phis)\n",
    "        particle_flow = np.stack([pts, etas, phis], axis=-1)  # (N, ΣM, 3)\n",
    "        \n",
    "        # -------- Mask array (N, ΣM) --------\n",
    "        mask = []\n",
    "        for channel in channels:\n",
    "            if 'mask' in hdf5_file[channel]:\n",
    "                _mask = np.asarray(hdf5_file[channel]['mask'][:], dtype=bool)  # (N, M)\n",
    "            else:\n",
    "                _mask = np.ones_like(hdf5_file[channel]['pt'][:], dtype=bool)\n",
    "            mask.append(_mask)\n",
    "        mask = np.concatenate(mask, axis=1)  # (N, ΣM)\n",
    "\n",
    "        # -------- Apply mask to particle flow --------\n",
    "        particle_flow = np.where(mask[..., None], particle_flow, np.nan)\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def _preprocess_phi_transformation(self, particle_flow: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transform phi to reduce variance (if var(phi) > 0.5, phi -> phi + pi).\"\"\"\n",
    "\n",
    "        phi = particle_flow[..., 2]\n",
    "        phi_var = np.var(np.nan_to_num(phi, nan=0.0), axis=-1, keepdims=True)\n",
    "        phi = np.where(phi_var > 0.5, phi + np.pi, phi)\n",
    "        phi = wrap_pi(phi)\n",
    "        particle_flow[..., 2] = phi\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def _preprocess_center_of_phi(self, particle_flow: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "        \"\"\"Shift phi to the center of pt frame.\"\"\"\n",
    "\n",
    "        pt = particle_flow[..., 0]  # (N, ΣM)\n",
    "        phi = particle_flow[..., 2]  # (N, ΣM)\n",
    "\n",
    "        pt_phi = np.nansum(pt * phi, axis=-1, keepdims=True)  # (N, 1)\n",
    "        center_of_phi = pt_phi / (np.nansum(pt, axis=-1, keepdims=True) + eps)  # (N, 1)\n",
    "        phi = wrap_pi(phi - center_of_phi)\n",
    "        particle_flow[..., 2] = phi\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def _preprocess_flipping(self, particle_flow: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Flip quadrant with highest pt to the first quadrant (phi > 0, eta > 0).\"\"\"\n",
    "\n",
    "        pt = particle_flow[..., 0]  # (N, ΣM)\n",
    "        eta = particle_flow[..., 1]  # (N, ΣM)\n",
    "        phi = particle_flow[..., 2]  # (N, ΣM)\n",
    "\n",
    "        # -------- Quadrant pT sums (0: ++, 1: +-, 2: --, 3: -+) --------\n",
    "        cond0 = (eta > 0) & (phi > 0)\n",
    "        cond1 = (eta > 0) & (phi < 0)\n",
    "        cond2 = (eta < 0) & (phi < 0)\n",
    "        cond3 = (eta < 0) & (phi > 0)\n",
    "        conds = np.stack([cond0, cond1, cond2, cond3], axis=-1)  # (N, ΣM, 4)\n",
    "        pt_quadrants = np.nansum(pt[..., None] * conds, axis=1)  # (N, 4)\n",
    "\n",
    "        # -------- Decide flips per event --------\n",
    "        q_argmax = np.argmax(pt_quadrants, axis=1)  # (N,)\n",
    "        phi_flip = np.where((q_argmax == 1) | (q_argmax == 2), -1.0, 1.0)[:, None]  # (N, 1)\n",
    "        eta_flip = np.where((q_argmax == 2) | (q_argmax == 3), -1.0, 1.0)[:, None]  # (N, 1)\n",
    "\n",
    "        # -------- Apply flips to particle flow --------\n",
    "        eta = eta * eta_flip\n",
    "        phi = wrap_pi(phi * phi_flip)\n",
    "        particle_flow[..., 1] = eta\n",
    "        particle_flow[..., 2] = phi\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def to_image(self, particle_flow: np.ndarray, include_decay: bool, grid_size: int = 40, eps: float=1e-8) -> torch.Tensor:\n",
    "        \"\"\"Convert the particle flow data to images (N, C, H, W).\"\"\"\n",
    "\n",
    "        particle_flow = np.where(np.isnan(particle_flow), 0.0, particle_flow)  # (N, ΣM, 3)\n",
    "\n",
    "        phi_bins = np.linspace(-np.pi, np.pi, grid_size + 1, dtype=np.float32)\n",
    "        eta_bins = np.linspace(-5.0, 5.0, grid_size + 1, dtype=np.float32)\n",
    "\n",
    "        def array_to_image(array) -> np.ndarray:\n",
    "            \"\"\"Convert one channel to image (N, H, W).\"\"\"\n",
    "\n",
    "            pt, eta, phi = array[..., 0], array[..., 1], array[..., 2]  # (N, M)\n",
    "\n",
    "            N, M = pt.shape\n",
    "            image = np.zeros((N, grid_size, grid_size), dtype=np.float32)\n",
    "\n",
    "            phi_idx = np.digitize(phi, phi_bins, right=False) - 1\n",
    "            eta_idx = np.digitize(eta, eta_bins, right=False) - 1\n",
    "            phi_idx = np.clip(phi_idx, 0, grid_size - 1)\n",
    "            eta_idx = np.clip(eta_idx, 0, grid_size - 1)\n",
    "\n",
    "            event_idx = np.repeat(np.arange(N, dtype=np.int64), M)\n",
    "            np.add.at(image, (event_idx, eta_idx.ravel(), phi_idx.ravel()), pt.ravel())\n",
    "\n",
    "            return image\n",
    "\n",
    "        images = []\n",
    "        if include_decay:\n",
    "            for _slice in self.slices:\n",
    "                array = particle_flow[:, _slice, :]  # (N, M, 3)\n",
    "                images.append(array_to_image(array))\n",
    "        else:\n",
    "            decay_image = array_to_image(particle_flow[:, self.slices[-1], :])\n",
    "            for i, channel in enumerate(self.detector_channels):\n",
    "                array = particle_flow[:, self.slices[i], :]  # (N, M, 3)\n",
    "                image = array_to_image(array)\n",
    "                if ('diphoton' in self.path and channel == 'TOWER') or ('zz4l' in self.path and channel == 'TRACK'):\n",
    "                    image = image - decay_image\n",
    "                images.append(image)\n",
    "        images = np.stack(images, axis=1)  # (N, C, H, W)\n",
    "\n",
    "        # --- pt normalisation per (N, C) across H*W ---\n",
    "        N, C, H, W = images.shape\n",
    "        flat = images.reshape(N, C, -1) \n",
    "        mean = flat.mean(axis=-1, keepdims=True) \n",
    "        std = flat.std(axis=-1, keepdims=True)\n",
    "        std = np.clip(std, a_min=eps, a_max=None)\n",
    "        images = (flat - mean) / std\n",
    "        images = images.reshape(N, C, H, W)\n",
    "\n",
    "        return torch.from_numpy(images).float()\n",
    "\n",
    "    def to_sequence(self, particle_flow: np.ndarray, include_decay: bool, eps: float = 0.0) -> torch.Tensor:\n",
    "        \"\"\"Convert particle flow features to sequences (N, ΣM_selected, 3+C).\"\"\"\n",
    "\n",
    "        # Choose which channel names / spans to emit\n",
    "        if include_decay:\n",
    "            channel_slices = self.slices\n",
    "        else:\n",
    "            channel_slices = self.slices[:-1]\n",
    "\n",
    "            # --- Remove detector hits that match decay objects (like _exclude_decay_information) ---\n",
    "            decay_slice = self.slices[-1]\n",
    "            decay_eta = particle_flow[:, decay_slice, 1]  # (N, M_dec)\n",
    "            decay_phi = particle_flow[:, decay_slice, 2]  # (N, M_dec)\n",
    "\n",
    "            # NaNs compare False in <= / ==, so NaN decay entries are ignored automatically\n",
    "            for detector_slice in channel_slices:\n",
    "                detector_eta = particle_flow[:, detector_slice, 1]  # (N, M_det)\n",
    "                detector_phi = particle_flow[:, detector_slice, 2]  # (N, M_det)\n",
    "\n",
    "                # broadcast (N, M_det, 1) vs (N, 1, M_dec) -> (N, M_det, M_dec)\n",
    "                eta_diff2 = (detector_eta[:, :, None] - decay_eta[:, None, :]) ** 2\n",
    "                phi_diff2 = (detector_phi[:, :, None] - decay_phi[:, None, :]) ** 2\n",
    "                dist2 = eta_diff2 + phi_diff2\n",
    "\n",
    "                if eps == 0.0:\n",
    "                    matched = (dist2 == 0.0).any(axis=-1)  # (N, M_det)\n",
    "                else:\n",
    "                    matched = (dist2 <= (eps * eps)).any(axis=-1)\n",
    "\n",
    "                # Set matched detector hits to NaN across [pt, eta, phi]\n",
    "                detector_view = particle_flow[:, detector_slice, :]\n",
    "                detector_view[matched] = np.nan\n",
    "\n",
    "        # --- Build sequences with per-event pt normalization and one-hot channel indicator ---\n",
    "        C = len(self.channels) if include_decay else len(self.detector_channels)\n",
    "        sequences = []\n",
    "\n",
    "        for one_hot_index, detector_slice in enumerate(channel_slices):\n",
    "            pt  = particle_flow[:, detector_slice, 0]  # (N, M)\n",
    "            eta = particle_flow[:, detector_slice, 1]  # (N, M)\n",
    "            phi = particle_flow[:, detector_slice, 2]  # (N, M)\n",
    "\n",
    "            # mask of valid hits for this channel\n",
    "            valid = ~np.isnan(pt)  # (N, M)\n",
    "\n",
    "            # per-event normalization (ignore NaNs)\n",
    "            pt_mean = np.nanmean(pt, axis=-1, keepdims=True)  # (N,1)\n",
    "            pt_std  = np.nanstd(pt,  axis=-1, keepdims=True)  # (N,1)\n",
    "            pt = (pt - pt_mean) / pt_std\n",
    "\n",
    "            feat = np.stack([pt, eta, phi], axis=-1)  # (N, M, 3)\n",
    "\n",
    "            # one-hot channel id\n",
    "            one_hot = np.zeros((1, 1, C), dtype=feat.dtype)  # (1,1,C)\n",
    "            one_hot[..., one_hot_index] = 1.0\n",
    "            one_hot = np.broadcast_to(one_hot, (feat.shape[0], feat.shape[1], C))\n",
    "            feat_oh = np.concatenate([feat, one_hot], axis=-1)  # (N, M, 3+C)\n",
    "\n",
    "            # keep NaNs where invalid\n",
    "            feat_oh = np.where(valid[..., None], feat_oh, np.nan)\n",
    "            sequences.append(feat_oh)\n",
    "\n",
    "        # concat channels along the sequence axis\n",
    "        sequences = np.concatenate(sequences, axis=1)  # (N, ΣM_selected, 3+C)\n",
    "\n",
    "        return torch.from_numpy(sequences).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e966180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(lightning.LightningDataModule):\n",
    "    def __init__(self, batch_size: int, data_mode: str, data_format: str, data_info: dict, \n",
    "                 include_decay: bool, luminosity: float = None, num_phi_augmentation: int = 0,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Monte Carlo simulation data\n",
    "        sig_data = MCSimData(project_root / data_info['signal']['path'])\n",
    "        bkg_data = MCSimData(project_root / data_info['background']['path'])\n",
    "\n",
    "        # Create mixed dataset for implementing CWoLa\n",
    "        if data_mode == 'jet_flavor':\n",
    "            train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg = self.split_by_jet_flavor(luminosity, data_info, sig_data, bkg_data)\n",
    "        elif data_mode == 'supervised':\n",
    "            train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg = self.split_by_supervised(sig_data, bkg_data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data mode: {data_mode}. Supported data modes are 'jet_flavor' and 'supervised'.\")\n",
    "        \n",
    "        # Data augmentation by random phi rotation\n",
    "        if num_phi_augmentation > 0:\n",
    "            train_sig = self.augment_phi_per_event(train_sig, num_phi_augmentation)\n",
    "            train_bkg = self.augment_phi_per_event(train_bkg, num_phi_augmentation)\n",
    "        \n",
    "        # Transform to desired data format\n",
    "        if data_format == 'image':\n",
    "            train_sig = sig_data.to_image(train_sig, include_decay)\n",
    "            train_bkg = bkg_data.to_image(train_bkg, include_decay)\n",
    "            valid_sig = sig_data.to_image(valid_sig, include_decay)\n",
    "            valid_bkg = bkg_data.to_image(valid_bkg, include_decay)\n",
    "            test_sig  = sig_data.to_image(test_sig, include_decay)\n",
    "            test_bkg  = bkg_data.to_image(test_bkg, include_decay)\n",
    "        elif data_format == 'sequence':\n",
    "            train_sig = sig_data.to_sequence(train_sig, include_decay)\n",
    "            train_bkg = bkg_data.to_sequence(train_bkg, include_decay)\n",
    "            valid_sig = sig_data.to_sequence(valid_sig, include_decay)\n",
    "            valid_bkg = bkg_data.to_sequence(valid_bkg, include_decay)\n",
    "            test_sig  = sig_data.to_sequence(test_sig, include_decay)\n",
    "            test_bkg  = bkg_data.to_sequence(test_bkg, include_decay)\n",
    "\n",
    "        # For tracking number of data samples\n",
    "        self.train_sig, self.train_bkg = train_sig, train_bkg\n",
    "        self.valid_sig, self.valid_bkg = valid_sig, valid_bkg\n",
    "        self.test_sig, self.test_bkg = test_sig, test_bkg\n",
    "\n",
    "        # Create torch datasets\n",
    "        self.train_dataset = TensorDataset(torch.cat([train_sig, train_bkg], dim=0), torch.cat([torch.ones(len(train_sig)), torch.zeros(len(train_bkg))], dim=0))\n",
    "        self.valid_dataset = TensorDataset(torch.cat([valid_sig, valid_bkg], dim=0), torch.cat([torch.ones(len(valid_sig)), torch.zeros(len(valid_bkg))], dim=0))\n",
    "        self.test_dataset  = TensorDataset(torch.cat([test_sig, test_bkg], dim=0), torch.cat([torch.ones(len(test_sig)), torch.zeros(len(test_bkg))], dim=0))\n",
    "\n",
    "        # Calculate positive weight for loss function\n",
    "        num_pos = len(train_sig)  # y == 1\n",
    "        num_neg = len(train_bkg)  # y == 0\n",
    "        self.pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float32)\n",
    "\n",
    "    def split_by_supervised(self, sig_data: MCSimData, bkg_data: MCSimData):\n",
    "        \"\"\"Split data for supervised training.\"\"\"\n",
    "\n",
    "        NUM_TRAIN, NUM_VALID, NUM_TEST = 100000, 25000, 25000\n",
    "\n",
    "        perm_sig = np.random.permutation(len(sig_data.particle_flow))\n",
    "        perm_bkg = np.random.permutation(len(bkg_data.particle_flow))\n",
    "        sig_array = sig_data.particle_flow[perm_sig]\n",
    "        bkg_array = bkg_data.particle_flow[perm_bkg]\n",
    "\n",
    "        train_sig = sig_array[:NUM_TRAIN]\n",
    "        train_bkg = bkg_array[:NUM_TRAIN]\n",
    "        valid_sig = sig_array[NUM_TRAIN: NUM_TRAIN + NUM_VALID]\n",
    "        valid_bkg = bkg_array[NUM_TRAIN: NUM_TRAIN + NUM_VALID]\n",
    "        test_sig = sig_array[NUM_TRAIN + NUM_VALID: NUM_TRAIN + NUM_VALID + NUM_TEST]\n",
    "        test_bkg = bkg_array[NUM_TRAIN + NUM_VALID: NUM_TRAIN + NUM_VALID + NUM_TEST]\n",
    "\n",
    "        return train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg\n",
    "\n",
    "    def split_by_jet_flavor(self, luminosity: float, data_info:dict, sig_data: MCSimData, bkg_data: MCSimData):\n",
    "        \"\"\"Split data by jet flavor composition for CWoLa training.\"\"\"\n",
    "\n",
    "        def get_event_counts(data_type: str, cut_info_key: str):\n",
    "            cut_info_path = project_root / data_info[data_type]['cut_info']\n",
    "            cut_info_npy = np.load(cut_info_path, allow_pickle=True)\n",
    "            cut_info = cut_info_npy.item()['cutflow_number']\n",
    "            N = int(data_info[data_type]['cross_section'] * cut_info[cut_info_key] / cut_info['Total'] * data_info['branching_ratio'] * luminosity)\n",
    "            print(f\"[CWoLa-Log] [{data_type}] {cut_info_key}: {N} events\")\n",
    "            return N\n",
    "\n",
    "        NUM_TEST = 10000\n",
    "        num_sig_in_sig = get_event_counts('signal', 'two quark jet: sig region')\n",
    "        num_sig_in_bkg = get_event_counts('signal', 'two quark jet: bkg region')\n",
    "        num_bkg_in_sig = get_event_counts('background', 'two quark jet: sig region')\n",
    "        num_bkg_in_bkg = get_event_counts('background', 'two quark jet: bkg region')\n",
    "\n",
    "        sig_in_sig_mask = sig_data.jet_flavor['2q0g']\n",
    "        sig_in_bkg_mask = sig_data.jet_flavor['1q1g'] | sig_data.jet_flavor['0q2g']\n",
    "        bkg_in_sig_mask = bkg_data.jet_flavor['2q0g']\n",
    "        bkg_in_bkg_mask = bkg_data.jet_flavor['1q1g'] | bkg_data.jet_flavor['0q2g']\n",
    "        num_test_sig_in_sig = int(NUM_TEST * np.sum(sig_in_sig_mask) / (np.sum(sig_in_sig_mask) + np.sum(sig_in_bkg_mask)))\n",
    "        num_test_sig_in_bkg = NUM_TEST - num_test_sig_in_sig\n",
    "        num_test_bkg_in_sig = int(NUM_TEST * np.sum(bkg_in_sig_mask) / (np.sum(bkg_in_sig_mask) + np.sum(bkg_in_bkg_mask)))\n",
    "        num_test_bkg_in_bkg = NUM_TEST - num_test_bkg_in_sig\n",
    "\n",
    "        sig_in_sig = sig_data.particle_flow[sig_in_sig_mask]\n",
    "        sig_in_bkg = sig_data.particle_flow[sig_in_bkg_mask]\n",
    "        bkg_in_sig = bkg_data.particle_flow[bkg_in_sig_mask]\n",
    "        bkg_in_bkg = bkg_data.particle_flow[bkg_in_bkg_mask]\n",
    "\n",
    "        idx_sig_in_sig = np.random.choice(len(sig_in_sig), num_sig_in_sig + num_test_sig_in_sig, replace=False)\n",
    "        idx_sig_in_bkg = np.random.choice(len(sig_in_bkg), num_sig_in_bkg + num_test_sig_in_bkg, replace=False)\n",
    "        idx_bkg_in_sig = np.random.choice(len(bkg_in_sig), num_bkg_in_sig + num_test_bkg_in_sig, replace=False)\n",
    "        idx_bkg_in_bkg = np.random.choice(len(bkg_in_bkg), num_bkg_in_bkg + num_test_bkg_in_bkg, replace=False)\n",
    "\n",
    "        sig_in_sig = sig_in_sig[idx_sig_in_sig]\n",
    "        sig_in_bkg = sig_in_bkg[idx_sig_in_bkg]\n",
    "        bkg_in_sig = bkg_in_sig[idx_bkg_in_sig]\n",
    "        bkg_in_bkg = bkg_in_bkg[idx_bkg_in_bkg]\n",
    "\n",
    "        def split_data(data: np.array, num_samples: int, num_test: int):\n",
    "            TRAIN_SIZE_RATIO = 0.8\n",
    "            train_size = int(num_samples * TRAIN_SIZE_RATIO)\n",
    "            return (\n",
    "                data[:train_size],\n",
    "                data[train_size:num_samples],\n",
    "                data[num_samples:num_samples + num_test]\n",
    "            )\n",
    "\n",
    "        train_sig_in_sig, valid_sig_in_sig, test_sig_in_sig = split_data(sig_in_sig, num_sig_in_sig, num_test_sig_in_sig)\n",
    "        train_sig_in_bkg, valid_sig_in_bkg, test_sig_in_bkg = split_data(sig_in_bkg, num_sig_in_bkg, num_test_sig_in_bkg)\n",
    "        train_bkg_in_sig, valid_bkg_in_sig, test_bkg_in_sig = split_data(bkg_in_sig, num_bkg_in_sig, num_test_bkg_in_sig)\n",
    "        train_bkg_in_bkg, valid_bkg_in_bkg, test_bkg_in_bkg = split_data(bkg_in_bkg, num_bkg_in_bkg, num_test_bkg_in_bkg)\n",
    "\n",
    "        train_sig = np.concatenate([train_sig_in_sig, train_bkg_in_sig], axis=0)\n",
    "        train_bkg = np.concatenate([train_sig_in_bkg, train_bkg_in_bkg], axis=0)\n",
    "        valid_sig = np.concatenate([valid_sig_in_sig, valid_bkg_in_sig], axis=0)\n",
    "        valid_bkg = np.concatenate([valid_sig_in_bkg, valid_bkg_in_bkg], axis=0)\n",
    "        test_sig  = np.concatenate([test_sig_in_sig, test_sig_in_bkg], axis=0)\n",
    "        test_bkg  = np.concatenate([test_bkg_in_sig, test_bkg_in_bkg], axis=0)\n",
    "\n",
    "        return train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg\n",
    "    \n",
    "    def augment_phi_per_event(self, data: np.ndarray, k: int) -> np.ndarray:\n",
    "        \"\"\"Augment data by random phi rotation per event, implemented in a memory-efficient way.\"\"\"\n",
    "\n",
    "        N, M, C = data.shape\n",
    "        out = np.empty(((k + 1) * N, M, C), dtype=data.dtype)\n",
    "        out[:N] = data\n",
    "        for i in range(k):\n",
    "            s = slice((i + 1) * N, (i + 2) * N)\n",
    "            out[s] = data\n",
    "            shift = np.random.uniform(-np.pi, np.pi, size=(N, 1)).astype(data.dtype)\n",
    "            out[s, :, 2] = wrap_pi(out[s, :, 2] + shift)\n",
    "        return out\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaadd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLitModel(lightning.LightningModule):\n",
    "    def __init__(self, model: nn.Module, lr: float, pos_weight: torch.Tensor = None, optimizer_settings: dict = None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.optimizer_settings = optimizer_settings\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        base_metrics = MetricCollection({\"auc\": BinaryAUROC(), \"accuracy\": BinaryAccuracy()})\n",
    "        self.metrics = {\n",
    "            \"train\": base_metrics.clone(prefix=\"train_\"),\n",
    "            \"valid\": base_metrics.clone(prefix=\"valid_\"),\n",
    "            \"test\": base_metrics.clone(prefix=\"test_\"),\n",
    "        }\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:        \n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_settings = self.optimizer_settings\n",
    "        optimizer = getattr(torch.optim, self.optimizer_settings['optimizer'])\n",
    "        optimizer = optimizer(self.parameters(), lr=self.lr)\n",
    "        if optimizer_settings['lr_scheduler'] is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            scheduler = getattr(torch.optim.lr_scheduler, optimizer_settings['lr_scheduler'])\n",
    "            scheduler = scheduler(optimizer, **optimizer_settings[scheduler.__name__])\n",
    "            lr_scheduler: dict = {'scheduler': scheduler}\n",
    "            lr_scheduler.update(optimizer_settings['lightning_monitor'])\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        \n",
    "    def on_fit_start(self):\n",
    "        # Move all metrics to the same device as the model\n",
    "        for split in self.metrics:\n",
    "            self.metrics[split] = self.metrics[split].to(self.device)\n",
    "\n",
    "    def _shared_step(self, batch: Tuple[torch.Tensor, torch.Tensor], split: str):\n",
    "        x, y_true = batch\n",
    "        logits    = self(x).squeeze(-1)\n",
    "        loss      = self.loss_fn(logits, y_true.float())\n",
    "        y_pred    = torch.sigmoid(logits)\n",
    "        self.metrics[split].update(y_pred, y_true.int())\n",
    "        self.log(f\"{split}_loss\", loss, on_epoch=True, on_step=False, prog_bar=(split == \"train\"), batch_size=y_true.size(0))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"test\")\n",
    "    \n",
    "    def _compute_and_log_split(self, split: str, prog_bar: bool = False):\n",
    "        computed = self.metrics[split].compute()\n",
    "        self.log_dict(computed, on_epoch=True, on_step=False, prog_bar=prog_bar)\n",
    "        self.metrics[split].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self._compute_and_log_split(\"train\", prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self._compute_and_log_split(\"valid\", prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self._compute_and_log_split(\"test\", prog_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d63a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_like_init(module: nn.Module):\n",
    "    # Dense / Conv weights = Glorot-uniform, biases = zeros\n",
    "    if isinstance(module, (nn.Linear,\n",
    "                           nn.Conv1d, nn.Conv2d, nn.Conv3d,\n",
    "                           nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d)):\n",
    "        if getattr(module, \"weight\", None) is not None:\n",
    "            nn.init.xavier_uniform_(module.weight)     # = Keras glorot_uniform\n",
    "        if getattr(module, \"bias\", None) is not None:\n",
    "            nn.init.zeros_(module.bias)                # = Keras zeros\n",
    "\n",
    "    # BatchNorm: gamma=1, beta=0 (running stats untouched here)\n",
    "    if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "        if getattr(module, \"weight\", None) is not None:\n",
    "            nn.init.ones_(module.weight)               # gamma\n",
    "        if getattr(module, \"bias\", None) is not None:\n",
    "            nn.init.zeros_(module.bias)                # beta\n",
    "\n",
    "    # LayerNorm: gamma=1, beta=0 (match Keras LayerNormalization scale/beta)\n",
    "    if isinstance(module, nn.LayerNorm):\n",
    "        if getattr(module, \"weight\", None) is not None:\n",
    "            nn.init.ones_(module.weight)\n",
    "        if getattr(module, \"bias\", None) is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def training(\n",
    "        data_mode: str, data_format: str, data_info: dict, include_decay: bool,\n",
    "        model_cls: nn.Module, lr: float, keras_init: bool, \n",
    "        tags: List[str], rnd_seed: int, date_time: str, **kwargs\n",
    "    ):\n",
    "    \"\"\"Train a model with the given configuration.\"\"\"\n",
    "\n",
    "    lightning.seed_everything(rnd_seed)\n",
    "\n",
    "    num_channels = 3 if include_decay else 2\n",
    "    model = model_cls(num_channels=num_channels, keras_init=keras_init)\n",
    "    if keras_init:\n",
    "        model.apply(keras_like_init)\n",
    "\n",
    "    # Output and log directories\n",
    "    save_dir = project_root / 'output' / (('' if include_decay else 'ex-') + data_info['decay_channel']) / ('_'.join(tags))\n",
    "    if data_mode == 'jet_flavor':\n",
    "        name = f\"{model.__class__.__name__}-{date_time}-L{kwargs['luminosity']}\"\n",
    "    elif data_mode == 'supervised':\n",
    "        name = f\"{model.__class__.__name__}-{date_time}-SV\"\n",
    "    version = f\"rnd_seed-{rnd_seed}\"\n",
    "    output_dir = save_dir / name / version\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"[Warning] Output directory {output_dir} already exists.\")\n",
    "        return\n",
    "\n",
    "    # Lightning data setup\n",
    "    BATCH_SIZE = 512\n",
    "    lit_data_module = LitDataModule(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        data_mode=data_mode,\n",
    "        data_format=data_format,\n",
    "        data_info=data_info,\n",
    "        include_decay=include_decay,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Lightning model setup\n",
    "    with open(project_root / 'config' / 'training.yml', 'r') as f:\n",
    "        training_config = yaml.safe_load(f)\n",
    "    lit_model = BinaryLitModel(\n",
    "        model=model,\n",
    "        lr=lr,\n",
    "        pos_weight=lit_data_module.pos_weight,\n",
    "        optimizer_settings=training_config['optimizer_settings']\n",
    "    )\n",
    "\n",
    "    # Lightning loggers\n",
    "    logger = CSVLogger(save_dir=save_dir, name=name, version=version)\n",
    "    hparams = {}\n",
    "    hparams.update(lit_data_module.hparams)\n",
    "    hparams.update(training_config)\n",
    "    logger.log_hyperparams(hparams)\n",
    "\n",
    "    # Lightning trainer\n",
    "    trainer = lightning.Trainer(\n",
    "        max_epochs=500,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        logger=logger,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(**training_config['ModelCheckpoint']),\n",
    "            EarlyStopping(**training_config['EarlyStopping']),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Lightning trainning and testing\n",
    "    trainer.fit(lit_model, lit_data_module)\n",
    "    trainer.test(lit_model, datamodule=lit_data_module, ckpt_path='best')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    utils.count_number_of_data(lit_data_module, output_dir)\n",
    "    utils.count_model_parameters(lit_model, output_dir)\n",
    "    utils.plot_metrics(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "KERAS_INIT = False\n",
    "N_AUG_ROT = 5\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    rnd_seeds = [123 + 100 * i for i in range(10)]\n",
    "\n",
    "    for rnd_seed, data_mode, include_decay in product(rnd_seeds, ['jet_flavor'], [True, False]):\n",
    "\n",
    "        with open(project_root / 'config' / 'data_diphoton.yml', 'r') as f:\n",
    "            data_info = yaml.safe_load(f)\n",
    "\n",
    "        for data_format, model_cls, lr in [\n",
    "            ('image', CNN_EventCNN, 1e-4),\n",
    "            # ('sequence', ParT_Light, 4e-4),\n",
    "        ]:\n",
    "\n",
    "            for luminosity in [100, 300, 900, 1800, 3000]:\n",
    "                training(\n",
    "                    data_mode=data_mode,\n",
    "                    data_format=data_format,\n",
    "                    data_info=data_info,\n",
    "                    include_decay=include_decay,\n",
    "                    model_cls=model_cls,\n",
    "                    lr=lr,\n",
    "                    keras_init=KERAS_INIT,\n",
    "                    tags=[data_mode],\n",
    "                    rnd_seed=rnd_seed,\n",
    "                    date_time=DATETIME,\n",
    "                    luminosity=luminosity,\n",
    "                    num_phi_augmentation=N_AUG_ROT,\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
