{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0698be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import h5py\n",
    "import lightning\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    '''Jupyter notebook environment has no __file__ attribute.'''\n",
    "    project_root = Path.cwd().parent\n",
    "sys.path.append(project_root.as_posix())\n",
    "\n",
    "from src.model_cnn import CNN_EventCNN\n",
    "from src.model_part import ParT_Light\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18650227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_pi(phi: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "    \"\"\"Wrap angles to [-pi, pi).\"\"\"\n",
    "    return (phi + np.pi) % (2 * np.pi) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ec348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCSimData:\n",
    "    def __init__(self, path: str | Path, include_decay=True):\n",
    "        self.path = str(path)\n",
    "        self.include_decay = include_decay\n",
    "\n",
    "        with h5py.File(str(path), 'r') as hdf5_file:\n",
    "            # -------- Jet flavor --------\n",
    "            self.jet_flavor = self._extract_jet_flavor(hdf5_file)\n",
    "\n",
    "            # -------- Channels --------\n",
    "            if 'diphoton' in self.path:\n",
    "                decay_channel = 'PHOTON'\n",
    "            elif 'zz4l' in self.path:\n",
    "                decay_channel = 'LEPTON'\n",
    "            detector_channels = ['TOWER', 'TRACK']\n",
    "            self.decay_channel = decay_channel\n",
    "            self.detector_channels = detector_channels\n",
    "\n",
    "            # -------- Particle flow information (pt, eta, phi) --------\n",
    "            particle_flow = self._extract_particle_flow(hdf5_file, detector_channels + [decay_channel])\n",
    "\n",
    "            # -------- Preprocessing --------\n",
    "            particle_flow = self._preprocess_phi_transformation(particle_flow)\n",
    "            particle_flow = self._preprocess_center_of_phi(particle_flow)\n",
    "            particle_flow = self._preprocess_flipping(particle_flow)\n",
    "            self.particle_flow = particle_flow\n",
    "\n",
    "    def _extract_jet_flavor(self, hdf5_file: h5py.File) -> Dict[str, torch.Tensor | int]:\n",
    "        \"\"\"Build gluon/quark composition masks from J1/J2 flavors.\"\"\"\n",
    "\n",
    "        J1 = torch.as_tensor(hdf5_file[\"J1\"][\"flavor\"][:], dtype=torch.long)\n",
    "        J2 = torch.as_tensor(hdf5_file[\"J2\"][\"flavor\"][:], dtype=torch.long)\n",
    "        g1, g2 = (J1 == 21), (J2 == 21)  # 21 == gluon\n",
    "\n",
    "        mask_2q0g = (~g1) & (~g2)\n",
    "        mask_1q1g = ((~g1) & g2) | (g1 & (~g2))\n",
    "        mask_0q2g = g1 & g2\n",
    "\n",
    "        return {\n",
    "            \"2q0g\": mask_2q0g,\n",
    "            \"1q1g\": mask_1q1g,\n",
    "            \"0q2g\": mask_0q2g,\n",
    "            \"total\": len(J1),\n",
    "        }\n",
    "    \n",
    "    def _extract_particle_flow(self, hdf5_file: h5py.File, channels: List[str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load pt/eta/phi/(mask) for each channel.\"\"\"\n",
    "\n",
    "        particle_flow: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        \n",
    "        for channel in channels:\n",
    "            pt  = np.asarray(hdf5_file[channel]['pt'][:],  dtype=np.float32)\n",
    "            eta = np.asarray(hdf5_file[channel]['eta'][:], dtype=np.float32)\n",
    "            phi = np.asarray(hdf5_file[channel]['phi'][:], dtype=np.float32)\n",
    "            phi = wrap_pi(phi)\n",
    "\n",
    "            entry = {'pt': pt, 'eta': eta, 'phi': phi}\n",
    "            if 'mask' in hdf5_file[channel]:\n",
    "                mask = np.asarray(hdf5_file[channel]['mask'][:], dtype=bool)\n",
    "            else:\n",
    "                mask = np.ones_like(pt, dtype=bool)\n",
    "            entry['mask'] = mask\n",
    "            particle_flow[channel] = entry\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def _exclude_decay_information(self, particle_flow: Dict[str, np.ndarray], eps: float = 0.) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Mask out detector hits that coincide with decay objects (exact eta/phi match).\"\"\"\n",
    "\n",
    "        decay_pt  = particle_flow[self.decay_channel]['pt']\n",
    "        decay_eta = particle_flow[self.decay_channel]['eta']\n",
    "        decay_phi = particle_flow[self.decay_channel]['phi']\n",
    "\n",
    "        for channel in self.detector_channels:\n",
    "            pt   = particle_flow[channel]['pt']\n",
    "            eta  = particle_flow[channel]['eta']\n",
    "            phi  = particle_flow[channel]['phi']\n",
    "            mask = particle_flow[channel]['mask']\n",
    "\n",
    "            eta_diff2 = (eta[:, :, np.newaxis] - decay_eta[:, np.newaxis, :]) ** 2\n",
    "            phi_diff2 = (phi[:, :, np.newaxis] - decay_phi[:, np.newaxis, :]) ** 2\n",
    "            dist2 = eta_diff2 + phi_diff2  # (N, M, M)\n",
    "\n",
    "            # A detector hit is \"matched to a decay\" if any decay object is within tolerance\n",
    "            if eps == 0.0:\n",
    "                matched = (dist2 == 0.0).any(axis=-1)  # (N, M)\n",
    "            else:\n",
    "                matched = (dist2 <= (eps ** 2)).any(axis=-1)  # (N, M)\n",
    "            non_decay = ~matched\n",
    "            mask = mask & non_decay\n",
    "            particle_flow[channel]['mask'] = mask\n",
    "            particle_flow[channel]['pt'] = np.where(mask, pt, 0.0)\n",
    "            particle_flow[channel]['eta'] = np.where(mask, eta, 0.0)\n",
    "            particle_flow[channel]['phi'] = np.where(mask, phi, 0.0)\n",
    "\n",
    "            # Diagnostic: 100% means exactly K decay matches per event have been excluded\n",
    "            num_decay_match = np.sum(non_decay, axis=-1) == (pt.shape[-1] - decay_pt.shape[-1])\n",
    "            purity = np.sum(num_decay_match) / num_decay_match.shape[0]\n",
    "            print(f'[{self.__class__.__name__} Log] {self.path}-{channel} has purity {100 * purity:.4f}%')\n",
    "\n",
    "        return particle_flow\n",
    "    \n",
    "    def _preprocess_phi_transformation(self, particle_flow: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Transform phi to reduce variance (if var(phi) > 0.5, phi -> phi + pi).\"\"\"\n",
    "\n",
    "        global_phi = np.concatenate([particle_flow[channel]['phi'] for channel in particle_flow], axis=-1)\n",
    "        global_phi_var = np.var(global_phi, axis=-1, keepdims=True)\n",
    "\n",
    "        for channel in particle_flow:\n",
    "            phi = particle_flow[channel]['phi']\n",
    "            phi = np.where(global_phi_var > 0.5, phi + np.pi, phi)\n",
    "            phi = wrap_pi(phi)\n",
    "            particle_flow[channel]['phi'] = phi\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def _preprocess_center_of_phi(self, particle_flow: Dict[str, np.ndarray], eps: float = 1e-8) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Shift phi to the center of pt frame.\"\"\"\n",
    "\n",
    "        global_pt = np.concatenate([particle_flow[channel]['pt'] for channel in particle_flow], axis=-1)\n",
    "        global_phi = np.concatenate([particle_flow[channel]['phi'] for channel in particle_flow], axis=-1)\n",
    "        global_pt_phi = np.sum(global_pt * global_phi, axis=-1, keepdims=True)\n",
    "        center_of_phi = global_pt_phi / (np.sum(global_pt, axis=-1, keepdims=True) + eps)\n",
    "\n",
    "        for channel in particle_flow:\n",
    "            phi = particle_flow[channel]['phi']\n",
    "            phi = phi - center_of_phi\n",
    "            phi = wrap_pi(phi)\n",
    "            particle_flow[channel]['phi'] = phi\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    def _preprocess_flipping(self, particle_flow: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Flip quadrant with highest pt to the first quadrant (phi > 0, eta > 0).\"\"\"\n",
    "\n",
    "        global_pt = np.concatenate([particle_flow[channel]['pt'] for channel in particle_flow], axis=-1)\n",
    "        global_phi = np.concatenate([particle_flow[channel]['phi'] for channel in particle_flow], axis=-1)\n",
    "        global_eta = np.concatenate([particle_flow[channel]['eta'] for channel in particle_flow], axis=-1)\n",
    "\n",
    "        # -------- Quadrant pT sums (0: ++, 1: +-, 2: --, 3: -+) --------\n",
    "        cond0 = (global_eta > 0) & (global_phi > 0)\n",
    "        cond1 = (global_eta > 0) & (global_phi < 0)\n",
    "        cond2 = (global_eta < 0) & (global_phi < 0)\n",
    "        cond3 = (global_eta < 0) & (global_phi > 0)\n",
    "        conds = np.stack([cond0, cond1, cond2, cond3], axis=-1)    # (N, ΣM, 4)\n",
    "        pt_quadrants = (global_pt[..., None] * conds).sum(axis=1)  # (N, 4)\n",
    "\n",
    "        # -------- Decide flips per event --------\n",
    "        q_argmax = np.argmax(pt_quadrants, axis=1)                 # (N,)\n",
    "        phi_flip = np.where((q_argmax == 1) | (q_argmax == 2), -1.0, 1.0)[:, None]  # (N,1)\n",
    "        eta_flip = np.where((q_argmax == 2) | (q_argmax == 3), -1.0, 1.0)[:, None]  # (N,1)\n",
    "\n",
    "        # -------- Apply flips to every channel --------\n",
    "        for channel in particle_flow:\n",
    "            particle_flow[channel]['eta'] = particle_flow[channel]['eta'] * eta_flip\n",
    "            particle_flow[channel]['phi'] = wrap_pi(particle_flow[channel]['phi'] * phi_flip)\n",
    "\n",
    "        return particle_flow\n",
    "\n",
    "    \n",
    "    def to_image(self, grid_size: int = 40, eps=1e-8) -> torch.Tensor:\n",
    "        \"\"\"Convert the particle flow data to images (N, C, H, W).\"\"\"\n",
    "\n",
    "        particle_flow = self.particle_flow.copy()\n",
    "\n",
    "        phi_bins = np.linspace(-np.pi, np.pi, grid_size + 1, dtype=np.float32)\n",
    "        eta_bins = np.linspace(-5.0, 5.0, grid_size + 1, dtype=np.float32)\n",
    "\n",
    "        def array_to_image(channel: str) -> np.ndarray:\n",
    "            \"\"\"Convert one channel to image (N, H, W).\"\"\"\n",
    "\n",
    "            mask = particle_flow[channel]['mask']  # (N, M)\n",
    "            pt   = np.where(mask, particle_flow[channel]['pt'], 0.0)   # (N, M)\n",
    "            eta  = np.where(mask, particle_flow[channel]['eta'], 0.0)  # (N, M)\n",
    "            phi  = np.where(mask, particle_flow[channel]['phi'], 0.0)  # (N, M)\n",
    "\n",
    "            N, M = pt.shape\n",
    "            image = np.zeros((N, grid_size, grid_size), dtype=np.float32)\n",
    "\n",
    "            phi_idx = np.digitize(phi, phi_bins, right=False) - 1\n",
    "            eta_idx = np.digitize(eta, eta_bins, right=False) - 1\n",
    "            phi_idx = np.clip(phi_idx, 0, grid_size - 1)\n",
    "            eta_idx = np.clip(eta_idx, 0, grid_size - 1)\n",
    "\n",
    "            event_idx = np.repeat(np.arange(N, dtype=np.int64), M)\n",
    "            np.add.at(image, (event_idx, phi_idx.ravel(), eta_idx.ravel()), pt.ravel())\n",
    "\n",
    "            return image\n",
    "\n",
    "        images = []\n",
    "        if self.include_decay:\n",
    "            for channel in self.detector_channels + [self.decay_channel]:\n",
    "                images.append(array_to_image(channel))\n",
    "        else:\n",
    "            decay_image = array_to_image(self.decay_channel)\n",
    "            for channel in self.detector_channels:\n",
    "                detector_image = array_to_image(channel)\n",
    "                images.append(np.where(decay_image == 0.0, detector_image, 0.0))\n",
    "        images = np.stack(images, axis=1)  # (N, C, H, W)\n",
    "\n",
    "        # --- pt normalisation per (N, C) across H*W ---\n",
    "        N, C, H, W = images.shape\n",
    "        flat = images.reshape(N, C, -1) \n",
    "        mean = flat.mean(axis=-1, keepdims=True) \n",
    "        std = flat.std(axis=-1, keepdims=True)\n",
    "        std = np.clip(std, a_min=eps, a_max=None)\n",
    "        images = (flat - mean) / std\n",
    "        images = images.reshape(N, C, H, W)\n",
    "\n",
    "        return torch.from_numpy(images).float()\n",
    "\n",
    "    def to_sequence(self, eps=1e-8) -> torch.Tensor:\n",
    "        \"\"\"Convert the particle flow data to sequences (N, ΣM, 3+|C|).\"\"\"\n",
    "\n",
    "        particle_flow = self.particle_flow.copy()\n",
    "\n",
    "        if self.include_decay:\n",
    "            channels = self.detector_channels + [self.decay_channel]\n",
    "        else:\n",
    "            particle_flow = self._exclude_decay_information(particle_flow, eps=0.0)\n",
    "            channels = self.detector_channels\n",
    "\n",
    "        seqs = []\n",
    "        C = len(channels)\n",
    "\n",
    "        for one_hot_index, channel in enumerate(channels):\n",
    "            mask = particle_flow[channel]['mask']\n",
    "            pt   = particle_flow[channel]['pt']\n",
    "            eta  = particle_flow[channel]['eta']\n",
    "            phi  = particle_flow[channel]['phi']\n",
    "\n",
    "            # --- pt normalization (per event) ---\n",
    "            pt_mean = np.mean(pt[mask], axis=-1, keepdims=True)\n",
    "            pt_std  = np.std(pt[mask], axis=-1, keepdims=True)\n",
    "            pt_std  = np.clip(pt_std, a_min=eps, a_max=None)\n",
    "            pt = (pt - pt_mean) / pt_std\n",
    "\n",
    "            # --- one-hot encoding ---\n",
    "            feat = np.stack([pt, eta, phi], axis=-1)  # (N, M, 3)\n",
    "            N, M, _ = feat.shape\n",
    "            one_hot = np.zeros((1, 1, C), dtype=feat.dtype)\n",
    "            one_hot[..., one_hot_index] = 1.0\n",
    "            one_hot = np.broadcast_to(one_hot, (N, M, C))\n",
    "            feat_oh = np.concatenate([feat, one_hot], axis=-1)  # (N, M, 3+C)\n",
    "\n",
    "            # --- masking ---\n",
    "            mask = particle_flow[channel]['mask'].astype(bool, copy=False)  # (N, M)\n",
    "            feat_oh = np.where(mask[..., None], feat_oh, np.nan)\n",
    "\n",
    "            seqs.append(feat_oh)\n",
    "\n",
    "        seqs = np.concatenate(seqs, axis=1)  # (N, ΣM, 3+C)\n",
    "        \n",
    "        return torch.from_numpy(seqs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e966180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(lightning.LightningDataModule):\n",
    "    def __init__(self, batch_size: int, data_mode: str, data_format: str, data_info: dict, \n",
    "                 include_decay: bool, luminosity: float = None, num_phi_augmentation: int = 0,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Monte Carlo simulation data\n",
    "        sig_data = MCSimData(project_root / data_info['signal']['path'], include_decay=include_decay)\n",
    "        bkg_data = MCSimData(project_root / data_info['background']['path'], include_decay=include_decay)\n",
    "\n",
    "        # Choose the representation of the dataset\n",
    "        if data_format == 'image':\n",
    "            self.sig_tensor, self.bkg_tensor = sig_data.to_image(), bkg_data.to_image()\n",
    "        elif data_format == 'sequence':\n",
    "            self.sig_tensor, self.bkg_tensor = sig_data.to_sequence(), bkg_data.to_sequence()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data format: {data_format}. Supported formats are 'image' and 'sequence'.\")\n",
    "\n",
    "        # Create mixed dataset for implementing CWoLa\n",
    "        if data_mode == 'jet_flavor':\n",
    "            train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg = self.split_by_jet_flavor(data_info=data_info, sig_flavor=sig_data.jet_flavor, bkg_flavor=bkg_data.jet_flavor)\n",
    "        elif data_mode == 'supervised':\n",
    "            train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg = self.split_by_supervised()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data mode: {data_mode}. Supported data modes are 'jet_flavor' and 'supervised'.\")\n",
    "\n",
    "        if num_phi_augmentation > 0:\n",
    "            train_sig = self.phi_augmentations(train_sig, num_phi_augmentation)\n",
    "            train_bkg = self.phi_augmentations(train_bkg, num_phi_augmentation)\n",
    "\n",
    "        # For tracking number of data samples\n",
    "        self.train_sig, self.train_bkg = train_sig, train_bkg\n",
    "        self.valid_sig, self.valid_bkg = valid_sig, valid_bkg\n",
    "        self.test_sig, self.test_bkg = test_sig, test_bkg\n",
    "\n",
    "        # Create torch datasets\n",
    "        self.train_dataset = TensorDataset(torch.cat([train_sig, train_bkg], dim=0), torch.cat([torch.ones(len(train_sig)), torch.zeros(len(train_bkg))], dim=0))\n",
    "        self.valid_dataset = TensorDataset(torch.cat([valid_sig, valid_bkg], dim=0), torch.cat([torch.ones(len(valid_sig)), torch.zeros(len(valid_bkg))], dim=0))\n",
    "        self.test_dataset  = TensorDataset(torch.cat([test_sig, test_bkg], dim=0), torch.cat([torch.ones(len(test_sig)), torch.zeros(len(test_bkg))], dim=0))\n",
    "\n",
    "        # Calculate positive weight for loss function\n",
    "        num_pos = len(train_sig)  # y == 1\n",
    "        num_neg = len(train_bkg)  # y == 0\n",
    "        self.pos_weight = torch.tensor([num_neg / num_pos], dtype=torch.float32)\n",
    "    \n",
    "    def phi_augmentations(self, data: torch.Tensor, rotations: int) -> torch.Tensor:\n",
    "\n",
    "        augmented_data = [data]\n",
    "\n",
    "        for _ in range(rotations):\n",
    "            new_data = data.clone()\n",
    "\n",
    "            if self.hparams.data_format == 'image':\n",
    "                shift = np.random.randint(1, new_data.shape[-2])\n",
    "                new_data = torch.roll(new_data, shifts=shift, dims=-2)\n",
    "            elif self.hparams.data_format == 'sequence':\n",
    "                phi_shift = 2 * np.pi * np.random.rand()\n",
    "                phi_column = 2\n",
    "                new_data[..., phi_column] = wrap_pi(new_data[..., phi_column] + phi_shift)\n",
    "            \n",
    "            augmented_data.append(new_data)\n",
    "\n",
    "        return torch.cat(augmented_data, dim=0)\n",
    "\n",
    "    def split_by_supervised(self):\n",
    "\n",
    "        NUM_TRAIN, NUM_VALID, NUM_TEST = 100000, 25000, 25000\n",
    "\n",
    "        sig_tensor = self.sig_tensor[torch.randperm(len(self.sig_tensor))]\n",
    "        bkg_tensor = self.bkg_tensor[torch.randperm(len(self.bkg_tensor))]\n",
    "        \n",
    "        train_sig = sig_tensor[:NUM_TRAIN]\n",
    "        train_bkg = bkg_tensor[:NUM_TRAIN]\n",
    "        valid_sig = sig_tensor[NUM_TRAIN: NUM_TRAIN + NUM_VALID]\n",
    "        valid_bkg = bkg_tensor[NUM_TRAIN: NUM_TRAIN + NUM_VALID]\n",
    "        test_sig = sig_tensor[NUM_TRAIN + NUM_VALID: NUM_TRAIN + NUM_VALID + NUM_TEST]\n",
    "        test_bkg = bkg_tensor[NUM_TRAIN + NUM_VALID: NUM_TRAIN + NUM_VALID + NUM_TEST]\n",
    "\n",
    "        return train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg\n",
    "\n",
    "    def split_by_jet_flavor(self, data_info:dict, sig_flavor: torch.Tensor, bkg_flavor: torch.Tensor):\n",
    "\n",
    "        def get_event_counts(data_type: str, cut_info_key: str):\n",
    "            cut_info_path = project_root / data_info[data_type]['cut_info']\n",
    "            cut_info_npy = np.load(cut_info_path, allow_pickle=True)\n",
    "            cut_info = cut_info_npy.item()['cutflow_number']\n",
    "            L = self.hparams.luminosity\n",
    "            N = int(data_info[data_type]['cross_section'] * cut_info[cut_info_key] / cut_info['Total'] * data_info['branching_ratio'] * L)\n",
    "            print(f\"[CWoLa-Log] [{data_type}] {cut_info_key}: {N} events\")\n",
    "            return N\n",
    "\n",
    "        NUM_TEST = 10000\n",
    "        num_sig_in_sig = get_event_counts('signal', 'two quark jet: sig region')\n",
    "        num_sig_in_bkg = get_event_counts('signal', 'two quark jet: bkg region')\n",
    "        num_bkg_in_sig = get_event_counts('background', 'two quark jet: sig region')\n",
    "        num_bkg_in_bkg = get_event_counts('background', 'two quark jet: bkg region')\n",
    "        num_test_sig_in_sig = int(NUM_TEST * num_sig_in_sig / (num_sig_in_sig + num_sig_in_bkg))\n",
    "        num_test_sig_in_bkg = NUM_TEST - num_test_sig_in_sig\n",
    "        num_test_bkg_in_sig = int(NUM_TEST * num_bkg_in_sig / (num_bkg_in_sig + num_bkg_in_bkg))\n",
    "        num_test_bkg_in_bkg = NUM_TEST - num_test_bkg_in_sig\n",
    "\n",
    "        sig_in_sig = self.sig_tensor[sig_flavor['2q0g']]\n",
    "        sig_in_bkg = self.sig_tensor[sig_flavor['1q1g'] | sig_flavor['0q2g']]\n",
    "        bkg_in_sig = self.bkg_tensor[bkg_flavor['2q0g']]\n",
    "        bkg_in_bkg = self.bkg_tensor[bkg_flavor['1q1g'] | bkg_flavor['0q2g']]\n",
    "\n",
    "        idx_sig_in_sig = np.random.choice(len(sig_in_sig), num_sig_in_sig + num_test_sig_in_sig, replace=False)\n",
    "        idx_sig_in_bkg = np.random.choice(len(sig_in_bkg), num_sig_in_bkg + num_test_sig_in_bkg, replace=False)\n",
    "        idx_bkg_in_sig = np.random.choice(len(bkg_in_sig), num_bkg_in_sig + num_test_bkg_in_sig, replace=False)\n",
    "        idx_bkg_in_bkg = np.random.choice(len(bkg_in_bkg), num_bkg_in_bkg + num_test_bkg_in_bkg, replace=False)\n",
    "\n",
    "        sig_in_sig = sig_in_sig[idx_sig_in_sig]\n",
    "        sig_in_bkg = sig_in_bkg[idx_sig_in_bkg]\n",
    "        bkg_in_sig = bkg_in_sig[idx_bkg_in_sig]\n",
    "        bkg_in_bkg = bkg_in_bkg[idx_bkg_in_bkg]\n",
    "\n",
    "        def split_data(data: torch.Tensor, num_samples: int, num_test: int):\n",
    "            TRAIN_SIZE_RATIO = 0.8\n",
    "            train_size = int(num_samples * TRAIN_SIZE_RATIO)\n",
    "            return (\n",
    "                data[:train_size],\n",
    "                data[train_size:num_samples],\n",
    "                data[num_samples:num_samples + num_test]\n",
    "            )\n",
    "\n",
    "        train_sig_in_sig, valid_sig_in_sig, test_sig_in_sig = split_data(sig_in_sig, num_sig_in_sig, num_test_sig_in_sig)\n",
    "        train_sig_in_bkg, valid_sig_in_bkg, test_sig_in_bkg = split_data(sig_in_bkg, num_sig_in_bkg, num_test_sig_in_bkg)\n",
    "        train_bkg_in_sig, valid_bkg_in_sig, test_bkg_in_sig = split_data(bkg_in_sig, num_bkg_in_sig, num_test_bkg_in_sig)\n",
    "        train_bkg_in_bkg, valid_bkg_in_bkg, test_bkg_in_bkg = split_data(bkg_in_bkg, num_bkg_in_bkg, num_test_bkg_in_bkg)\n",
    "\n",
    "        train_sig = torch.cat([train_sig_in_sig, train_bkg_in_sig], dim=0)\n",
    "        train_bkg = torch.cat([train_sig_in_bkg, train_bkg_in_bkg], dim=0)\n",
    "        valid_sig = torch.cat([valid_sig_in_sig, valid_bkg_in_sig], dim=0)\n",
    "        valid_bkg = torch.cat([valid_sig_in_bkg, valid_bkg_in_bkg], dim=0)\n",
    "        test_sig  = torch.cat([test_sig_in_sig, test_sig_in_bkg], dim=0)\n",
    "        test_bkg  = torch.cat([test_bkg_in_sig, test_bkg_in_bkg], dim=0)\n",
    "\n",
    "        return train_sig, train_bkg, valid_sig, valid_bkg, test_sig, test_bkg\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.hparams.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaadd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLitModel(lightning.LightningModule):\n",
    "    def __init__(self, model: nn.Module, lr: float, pos_weight: torch.Tensor = None, optimizer_settings: dict = None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.optimizer_settings = optimizer_settings\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        base_metrics = MetricCollection({\"auc\": BinaryAUROC(), \"accuracy\": BinaryAccuracy()})\n",
    "        self.metrics = {\n",
    "            \"train\": base_metrics.clone(prefix=\"train_\"),\n",
    "            \"valid\": base_metrics.clone(prefix=\"valid_\"),\n",
    "            \"test\": base_metrics.clone(prefix=\"test_\"),\n",
    "        }\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:        \n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_settings = self.optimizer_settings\n",
    "        optimizer = getattr(torch.optim, self.optimizer_settings['optimizer'])\n",
    "        optimizer = optimizer(self.parameters(), lr=self.lr)\n",
    "        if optimizer_settings['lr_scheduler'] is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            scheduler = getattr(torch.optim.lr_scheduler, optimizer_settings['lr_scheduler'])\n",
    "            scheduler = scheduler(optimizer, **optimizer_settings[scheduler.__name__])\n",
    "            lr_scheduler: dict = {'scheduler': scheduler}\n",
    "            lr_scheduler.update(optimizer_settings['lightning_monitor'])\n",
    "            return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n",
    "        \n",
    "    def on_fit_start(self):\n",
    "        # Move all metrics to the same device as the model\n",
    "        for split in self.metrics:\n",
    "            self.metrics[split] = self.metrics[split].to(self.device)\n",
    "\n",
    "    def _shared_step(self, batch: Tuple[torch.Tensor, torch.Tensor], split: str):\n",
    "        x, y_true = batch\n",
    "        logits    = self(x).squeeze(-1)\n",
    "        loss      = self.loss_fn(logits, y_true.float())\n",
    "        y_pred    = torch.sigmoid(logits)\n",
    "        self.metrics[split].update(y_pred, y_true.int())\n",
    "        self.log(f\"{split}_loss\", loss, on_epoch=True, on_step=False, prog_bar=(split == \"train\"), batch_size=y_true.size(0))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, \"test\")\n",
    "    \n",
    "    def _compute_and_log_split(self, split: str, prog_bar: bool = False):\n",
    "        computed = self.metrics[split].compute()\n",
    "        self.log_dict(computed, on_epoch=True, on_step=False, prog_bar=prog_bar)\n",
    "        self.metrics[split].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self._compute_and_log_split(\"train\", prog_bar=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self._compute_and_log_split(\"valid\", prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self._compute_and_log_split(\"test\", prog_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_like_init(module: nn.Module):\n",
    "    # Conv / Linear → Glorot-uniform (aka Xavier-uniform), bias zeros\n",
    "    if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d,\n",
    "                           nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d,\n",
    "                           nn.Linear)):\n",
    "        if module.weight is not None:\n",
    "            nn.init.xavier_uniform_(module.weight)   # Keras: glorot_uniform\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)              # Keras: zeros\n",
    "\n",
    "    # BatchNorm → gamma=1, beta=0 (running stats are already mean=0, var=1 in PyTorch)\n",
    "    if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "        if module.weight is not None:\n",
    "            nn.init.ones_(module.weight)             # gamma\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)              # beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d63a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "\n",
    "def training(\n",
    "        data_mode: str, data_format: str, data_info: dict, include_decay: bool,\n",
    "        model_cls: nn.Module, lr: float, tags: List[str], rnd_seed: int, **kwargs\n",
    "    ):\n",
    "    \"\"\"Train a model with the given configuration.\"\"\"\n",
    "\n",
    "    lightning.seed_everything(rnd_seed)\n",
    "\n",
    "    num_channels = 3 if include_decay else 2\n",
    "    model = model_cls(num_channels=num_channels)\n",
    "    model.apply(keras_like_init)\n",
    "\n",
    "    # Output and log directories\n",
    "    save_dir = project_root / 'output' / (('' if include_decay else 'ex-') + data_info['decay_channel']) / ('_'.join(tags))\n",
    "    if data_mode == 'jet_flavor':\n",
    "        name = f\"{model.__class__.__name__}-{DATETIME}-L{kwargs['luminosity']}\"\n",
    "    elif data_mode == 'supervised':\n",
    "        name = f\"{model.__class__.__name__}-{DATETIME}-SV\"\n",
    "    version = f\"rnd_seed-{rnd_seed}\"\n",
    "    output_dir = save_dir / name / version\n",
    "\n",
    "    # Lightning data setup\n",
    "    BATCH_SIZE = 512\n",
    "    lit_data_module = LitDataModule(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        data_mode=data_mode,\n",
    "        data_format=data_format,\n",
    "        data_info=data_info,\n",
    "        include_decay=include_decay,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Lightning model setup\n",
    "    with open(project_root / 'config' / 'training.yml', 'r') as f:\n",
    "        training_config = yaml.safe_load(f)\n",
    "    lit_model = BinaryLitModel(\n",
    "        model=model,\n",
    "        lr=lr,\n",
    "        pos_weight=lit_data_module.pos_weight,\n",
    "        optimizer_settings=training_config['optimizer_settings']\n",
    "    )\n",
    "\n",
    "    # Lightning loggers\n",
    "    logger = CSVLogger(save_dir=save_dir, name=name, version=version)\n",
    "    hparams = {}\n",
    "    hparams.update(lit_data_module.hparams)\n",
    "    hparams.update(training_config)\n",
    "    logger.log_hyperparams(hparams)\n",
    "\n",
    "    # Lightning trainer\n",
    "    trainer = lightning.Trainer(\n",
    "        max_epochs=200,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        logger=logger,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(**training_config['ModelCheckpoint']),\n",
    "            EarlyStopping(**training_config['EarlyStopping']),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Lightning trainning and testing\n",
    "    trainer.fit(lit_model, lit_data_module)\n",
    "    trainer.test(lit_model, datamodule=lit_data_module, ckpt_path='best')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    utils.count_number_of_data(lit_data_module, output_dir)\n",
    "    utils.count_model_parameters(lit_model, output_dir)\n",
    "    utils.plot_metrics(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    rnd_seeds = [23 + 100 * i for i in range(10)]\n",
    "\n",
    "    for rnd_seed, data_mode, include_decay in product(rnd_seeds, ['jet_flavor'], [True, False]):\n",
    "\n",
    "        with open(project_root / 'config' / 'data_diphoton.yml', 'r') as f:\n",
    "            data_info = yaml.safe_load(f)\n",
    "\n",
    "        for data_format, model_cls, lr in [\n",
    "            ('image', CNN_EventCNN, 1e-4),\n",
    "            # ('sequence', ParT_Light, 4e-4),\n",
    "        ]:\n",
    "\n",
    "            for luminosity in [100, 300, 900, 1800, 3000]:\n",
    "                training(\n",
    "                    data_mode=data_mode,\n",
    "                    data_format=data_format,\n",
    "                    data_info=data_info,\n",
    "                    include_decay=include_decay,\n",
    "                    model_cls=model_cls,\n",
    "                    lr=lr,\n",
    "                    tags=[data_mode],\n",
    "                    rnd_seed=rnd_seed,\n",
    "                    luminosity=luminosity,\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
